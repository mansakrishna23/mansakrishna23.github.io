---
layout: post
title: Blog Post 5 - Distinguishing Between Cats and Dogs Ft. TensorFlow
---
Distinguishing between cats and dogs seems like the easiest task in the world. And yet, training a machine learning model to do so is a non-trivial problem. As such, in this blog post, we will be learning several new skills, methods, and concepts related to image classification in TensorFlow, an open-source library for machine learning and artificial intelligence. 

### Acknowledgements
Many parts of this blog post, including chunks of code, are based on the TensorFlow [Transfer Learning Tutorial](https://www.tensorflow.org/tutorials/images/transfer_learning). 

### Loading Packages and Obtaining the Data
We will start be creating a block of code that will hold our import statements. Feel free to import any other packages you deem necessary!
```python
import os
import tensorflow as tf
from tensorflow.keras import utils
```
As always, our first step in any data science/machine learning project is to acquire the data and load it into Python. We will be using a sample data set containing labeled images of cats and dogs, provided by the TensorFlow team.

By running the following block of code, we create a Tensorflow data set for training, validating, and testing our machine learning model(s). In this case, we use data sets since it is not all that practical to load this data into memory. 
```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# extracting the class names
class_names = train_dataset.class_names

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```


{::options parse_block_html="true" /}
<div class="gave-help">
As far as this particular project was concerned, I definitely learned a lot about working with matrices and vectors in Python. It was a lot of fun to see some of the mathematical princples of linear algebra at work! I particularly enjoyed having in-built functions perform all those linear algebra computations for me (Having taken linear algebra classes, I know how stressful performing these computations by hand can be). 

Though Python has in-built functions that can perform these linear algebra computations for you - it can be really easy to make mistakes! Throughout the entire coding process, I learned how important it was to check whether I was multiplying the matrices correctly (i.e., multiplying the matrices in the correct order) or check whether I was constructing the vectors correctly. I definitely ran into errors while performing these computations because I had made minor errors while constructing these objects. 

All in all, this was a super fun project and I got to learn a lot!
</div>
{::options parse_block_html="false" /}
