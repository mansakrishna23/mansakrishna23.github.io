---
layout: post
title: Blog Post 6 - Building a Fake News Classifier ft. Tensorflow
---
In this blog post, i will be outlining how we will develop and assess a fake news classifier (a classification machine learning model).

### Acquiring the Data
As always, our first step is to acquire and load in the data. Our data source for this project is taken from the following article: 

>Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).

Let's read our data into Python!


```python
# Loading in the required packages
import pandas as pd
import numpy as np
```


```python
# URL at which data is located
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"

# reading our data into python!
data = pd.read_csv(train_url)
```

Remember, it is good practice to make sure your data is read in correctly by checking the first few rows!


```python
data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



Note that the columns that are relevant to us are the `title`, `text`, and `fake` columns. So, we can drop our `Unnamed: 0` columns as follows:


```python
data = data.drop('Unnamed: 0', axis=1)

# checking our data
data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



Well done! We have successfully acquired our data!

### Making Datasets
To make our data a little more concise is to remove stopwords from the article `text` and `title`. The reason we want to do this is because stopwords are words that are usually considered to be uninformative, such as “the” or “and”, etc. We also want to construct and return a tensorflow dataset with two inputs and one output: The inputs should be the title and texts while the output should be the `fake` column.

Let's write a function called `make_dataset` that should perform the aforementioned tasks. We will find it helpful to import the `feature_extraction` module from the `scikit-learn` library as well as the `tensorflow` library!


```python
from sklearn.feature_extraction import text
import tensorflow as tf
```

We will need a comprehensive list of stop words to eliminate!


```python
# Obtaining a comprehensive list of stopwords
stop = text.ENGLISH_STOP_WORDS
```

Now that we have imported the relevant packages, let's go ahead and write our function!


```python
def make_dataset(data):
    """
    Function removes the stopwords from the title and 
    text inputs and constructs a tensorflow dataset with 
    two inputs (title, text) and one output (fake). 
    
    Parameters
    ----------
    data : pd.DataFrame
        the original raw dataset with title, text, and fake
        columns
        
    Returns
    ----------
    tf.data.Dataset
        A tensorflow dataset with two inputs (title, text)
        and one output (fake)
    """
    # Removing the stop words from the title column
    titles = list(data['title'])
    new_titles = []
    
    for title in titles:
        # extracting only the relevant words
        relevant = [word for word in title.split(" ") if word not in stop]
        # Joining all relevant words into a single string
        new_title  = " ".join(relevant)
        # Appending new title to array
        new_titles.append(new_title)
        
    # Removing stopwords from the text column
    texts = list(data['text'])
    new_texts = []
    
    for text in texts:
        # extracting only relevant words
        relevant = [word for word in text.split(" ") if word not in stop]
        # Joining all relevant words into a single string
        new_text  = " ".join(relevant)
        # Appending new title to array
        new_texts.append(new_text)
    
    # Creating the tensorflow dataset
    df = tf.data.Dataset.from_tensor_slices(
        (
            {
                "title" : new_titles,
                "text" : new_texts
            },
            {
                "fake" : data[['fake']]
            }
        )
    )
    # Returning the tensorflow dataset
    return df
```

Well, we've written our function. So, let's go ahead and call the function on our training dataframe `data` to product a tensorflow `Dataset`. 

Note that we will be batching our dataset prior to returning it - this will enable our model to train on several rows of data rather than the individual rows. It is true that this might decrease the accuracy of our model, however it defintely speeds up the model training process!


```python
data = make_dataset(dataset).batch(100)
```
```python
data
```
    <BatchDataset shapes: ({title: (None,), text: (None,)}, {fake: (None,)}), types: ({title: tf.string, text: tf.string}, {fake: tf.int32})>
    
Now that we have constructed our primary Dataset, let us split of 20% of it to use for validation!
```python
data = data.shuffle(buffer_size = len(data))

train_size = int(0.8*len(data))
val_size   = int(0.2*len(data))

train = data.take(train_size).batch(20)
val   = data.skip(train_size).take(val_size).batch(20)
test  = data.skip(train_size + val_size).batch(20)

len(train), len(val), len(test)
```
    (9, 3, 0)
    
We will now examone our training set labels in order to determine the base rate for our data set!
*Recall that the base rate refers to the accuracy of a model that always makes the same guess.*

### Create Models

### Model Evaluation

### Embedding Visualization

{::options parse_block_html="true" /}
<div class="gave-help">
As far as this particular project was concerned, I definitely learned a lot more about using the tensorflow library!
</div>
{::options parse_block_html="false" /}
